#!/bin/bash

# Verify the DeepSeek Companion files are present
if [ -d "src/ai/deepseek-companion" ]; then
    echo "‚úÖ DeepSeek Companion files found"
else
    echo "‚ùå DeepSeek Companion files not found. Please ensure the integration is complete."
    exit 1
fi

# Download DeepSeek Companion models locally
MODEL_DIR="src/ai/deepseek-companion/models"
mkdir -p $MODEL_DIR

# Download models
MODELS=(
    "deepseek-ai/deepseek-companion-10b-base"
    "deepseek-ai/deepseek-companion-20b-instruct"
)

for MODEL in "${MODELS[@]}"; do
    echo "üì• Downloading model: $MODEL..."
    python3 -c "
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

print(f'üì• Downloading {MODEL}')
AutoTokenizer.from_pretrained('$MODEL').save_pretrained('$MODEL_DIR/$MODEL')
AutoModelForCausalLM.from_pretrained('$MODEL').save_pretrained('$MODEL_DIR/$MODEL')
"

    if [ $? -eq 0 ]; then
        echo "‚úÖ Model $MODEL downloaded successfully"
    else
        echo "‚ùå Failed to download model $MODEL"
        exit 1
    fi

done

# Verify models
if [ -d "$MODEL_DIR" ]; then
    echo "‚úÖ All models downloaded and stored in $MODEL_DIR"
else
    echo "‚ùå Model directory not found. Please check the setup."
    exit 1
fi

# Initialize the DeepSeek Companion service
echo "üîß Testing DeepSeek Companion integration..."
python3 -c "
import torch
from transformers import AutoTokenizer
print('‚úÖ DeepSeek Companion dependencies verified successfully!')
print(f'üî• CUDA available: {torch.cuda.is_available()}')
print(f'üî• Device count: {torch.cuda.device_count() if torch.cuda.is_available() else 0}')
"

if [ $? -eq 0 ]; then
    echo "‚úÖ DeepSeek Companion setup completed successfully"
else
    echo "‚ùå DeepSeek Companion setup failed"
    exit 1
fi
